{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95c995cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/antoineguines/.local/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/antoineguines/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/antoineguines/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/antoineguines/.local/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d4ae403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e38d671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0699a48",
   "metadata": {},
   "source": [
    "# I- Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3d4b2fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"your_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3b111d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 11)\n",
      "         x1        x2          x3        x4        x5        x6        x7  \\\n",
      "0  1.205492  5.823226   98.837539 -1.075852  0.999205  0.911543  3.623558   \n",
      "1  1.391530  3.611581   98.857197 -5.020318  0.677165  0.999492  3.413112   \n",
      "2  1.692571 -0.887019  100.901276 -0.595548  0.177550 -0.915495  4.320264   \n",
      "3  4.289320  1.416843  100.784735 -2.897154 -0.066972 -0.786173  2.093003   \n",
      "4  0.542420 -1.010095  100.015580 -3.070705  0.088324 -0.242669  0.767942   \n",
      "\n",
      "         x8        x9        x10  y  \n",
      "0 -1.720267 -0.346191 -54.708330 -1  \n",
      "1  4.253865  2.041603 -54.317291  1  \n",
      "2  0.907834  3.126815 -56.397484 -1  \n",
      "3  1.336237  2.183829 -56.197728  1  \n",
      "4 -0.284683 -2.104145 -55.794045  1  \n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e7ae8cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no missing values in the dataframe\n"
     ]
    }
   ],
   "source": [
    "def check_missing_values(df):\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.any():\n",
    "        print(\"There are missing values in the dataframe.\")\n",
    "        print(missing_values)\n",
    "    else:\n",
    "        print(\"There are no missing values in the dataframe\")\n",
    "\n",
    "# Appel de la fonction\n",
    "check_missing_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5f618b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that normalise the dataframe\n",
    "def normalise_df(df):\n",
    "    \n",
    "    df_norm = (df - df.min()) / (df.max() - df.min())\n",
    "    \n",
    "    return df_norm\n",
    "\n",
    "# function that splits the dataframe s.t : train set = 60% - val set = 20% - test set = 20%\n",
    "def split_dataframe(df_x, df_y):\n",
    "    \n",
    "    split = int(df.shape[0] * 0.8)\n",
    "    \n",
    "    X_train = df_x.iloc[:split,:]\n",
    "    Y_train = df_y.iloc[:split]\n",
    "\n",
    "    X_test = df_x.iloc[split:,:]\n",
    "    Y_test = df_y.iloc[split:]\n",
    "    \n",
    "    X_train = X_train.to_numpy()\n",
    "    X_test = X_test.to_numpy()\n",
    "    Y_train = Y_train.to_numpy()\n",
    "    Y_test = Y_test.to_numpy()\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "\n",
    "def save_set(X_train, Y_train, X_test, Y_test):\n",
    "    np.save('X_train.npy', X_train)\n",
    "    np.save('Y_train.npy', Y_train)\n",
    "    np.save('X_test.npy', X_test)\n",
    "    np.save('Y_test.npy', Y_test)\n",
    "    \n",
    "\n",
    "def pre_process_df(df):\n",
    "    \n",
    "    df = df.sample(frac=1).reset_index(drop=True) # mixed the dataframe\n",
    "    \n",
    "    X = df.iloc[:, :-1]\n",
    "    Y = df.iloc[:, -1]\n",
    "    \n",
    "    X_norm = normalise_df(X)\n",
    "    X_norm.insert(0,'bias',1)\n",
    "    X_train, Y_train, X_test, Y_test = split_dataframe(X_norm, Y)\n",
    "    save_set(X_train, Y_train, X_test, Y_test)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4341e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = pre_process_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1466dbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 11)\n",
      "(8000,)\n",
      "(2000, 11)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2c67e7",
   "metadata": {},
   "source": [
    "# II- Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6028b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_expansion_2(X):\n",
    "    \n",
    "    X_exp = X.copy()\n",
    "    \n",
    "    # add square product\n",
    "    for i in range(1, X.shape[1]):\n",
    "        square_feature = (X[:, i] ** 2).reshape(X.shape[0], 1)\n",
    "        X_exp = np.concatenate((X_exp, square_feature), axis=1)\n",
    "    \n",
    "    # add cross product\n",
    "    for i in range(1, X.shape[1]):\n",
    "        for j in range(i+1, X.shape[1]):\n",
    "            cross_feature = (X[:, i] * X[:, j]).reshape(X.shape[0], 1)\n",
    "            X_exp = np.concatenate((X_exp, cross_feature), axis=1)\n",
    "    \n",
    "    return X_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f7f3491",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('X_train.npy')\n",
    "Y_train = np.load('Y_train.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "Y_test = np.load('Y_test.npy')\n",
    "X_train_exp = feature_expansion_2(X_train)\n",
    "X_test_exp = feature_expansion_2(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c40e1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, feature_number=11):\n",
    "        self.max_epoch = None\n",
    "        self.feature_number = feature_number\n",
    "        self.w = None\n",
    "    \n",
    "    def train(self, X_train, Y_train, param):\n",
    "        \n",
    "        self.w = np.zeros(self.feature_number)\n",
    "        self.max_epoch = param\n",
    "\n",
    "        for epoch in range(self.max_epoch):\n",
    "            \n",
    "            #we start an EPOCH\n",
    "            update = False\n",
    "            for i in range(X_train.shape[0]):\n",
    "\n",
    "                dot_prod = Y_train[i] * np.dot(self.w, X_train[i,:])\n",
    "\n",
    "                if (dot_prod <= 0):\n",
    "                    self.w = self.w + Y_train[i] * X_train[i,:]\n",
    "                    update = True\n",
    "\n",
    "            if (update == False):\n",
    "                break\n",
    "        \n",
    "        return self.w\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        output = np.dot(X_test, self.w)\n",
    "        Y_predicted = np.sign(output)\n",
    "        \n",
    "        return Y_predicted\n",
    "    \n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        \n",
    "        Y_pred = self.predict(X_test)\n",
    "        accuracy = np.mean(Y_pred == Y_test)\n",
    "        \n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "68214a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spliting_for_tuning_param(X_train, Y_train):\n",
    "    split_idx = int(0.75 * X_train.shape[0])  # 75% for training, 25% for validation\n",
    "    X_train_tuning = X_train[:split_idx, :]\n",
    "    Y_train_tuning = Y_train[:split_idx]\n",
    "    X_val_tuning = X_train[split_idx:, :]\n",
    "    Y_val_tuning = Y_train[split_idx:]\n",
    "    \n",
    "    return X_train_tuning, Y_train_tuning, X_val_tuning, Y_val_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cc74d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_tuning_parameter(p, list_param, X_train_tuning, Y_train_tuning, X_val_tuning, Y_val_tuning):\n",
    "    \n",
    "    result = np.zeros(len(list_param))\n",
    "    for i, param in enumerate(list_param):\n",
    "        p.train(X_train_tuning, Y_train_tuning, param)\n",
    "        result[i] = p.evaluate(X_val_tuning, Y_val_tuning)\n",
    "\n",
    "    index_max = np.argmax(result)\n",
    "    best_param = list_param[index_max]\n",
    "    \n",
    "    return best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5618035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tuning, Y_train_tuning, X_val_tuning, Y_val_tuning = spliting_for_tuning_param(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "97a0095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tuning_exp, Y_train_tuning_exp, X_val_tuning_exp, Y_val_tuning_exp = spliting_for_tuning_param(X_train_exp, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7e2e5e",
   "metadata": {},
   "source": [
    "## a) Perceptron run without feature expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457c66d1",
   "metadata": {},
   "source": [
    "First we choose, among a define class, the best number of epoch to be carried out thanks to the function tuning_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "61514c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "list_epoch_param = [i for i in range(1, 31)]\n",
    "best_epoch_p = perceptron_tuning_parameter(Perceptron(11), list_epoch_param, X_train_tuning, Y_train_tuning, X_val_tuning, Y_val_tuning)\n",
    "print(best_epoch_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca5e542",
   "metadata": {},
   "source": [
    "Then we train the Perceptron using this number of epoch on the full train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5dadd1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [-3.          2.39717537  0.16794903 -1.31400493 -4.88529671  1.83689138\n",
      " -1.24603313  4.16327828  9.1343608   1.84258619 -1.37851589]\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron(11)\n",
    "w_perceptron = perceptron.train(X_train, Y_train, best_epoch_p)\n",
    "print('w =', w_perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0fff8b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rate of good answer on train set :  0.715375\n",
      "rate of good answer on test set :  0.7295\n"
     ]
    }
   ],
   "source": [
    "accuracy_train = perceptron.evaluate(X_train, Y_train)\n",
    "accuracy_test = perceptron.evaluate(X_test, Y_test)\n",
    "print('rate of good answer on train set : ', accuracy_train)\n",
    "print('rate of good answer on test set : ', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d985680",
   "metadata": {},
   "source": [
    "## b) Perceptron run with feature expansion of degree 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de907b",
   "metadata": {},
   "source": [
    "We proceed in the same way when feature expansion. We first choose the best number of epoch to be carried out :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6e9280d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "best_epoch_pfe = perceptron_tuning_parameter(Perceptron(66), list_epoch_param, X_train_tuning_exp, Y_train_tuning_exp, X_val_tuning_exp, Y_val_tuning_exp)\n",
    "print(best_epoch_pfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceeada3",
   "metadata": {},
   "source": [
    "And then we train our algorithm on the whole training set with feature expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "31a7cb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_exp = [ 2.60000000e+01 -3.84548191e+01 -1.14367796e+02  2.45083154e+01\n",
      "  3.79171999e+01 -2.70422837e+00  3.05210273e+00 -1.63408251e+01\n",
      "  1.61352483e+01 -9.64159823e+01  4.86890852e+00 -1.63309062e+01\n",
      "  2.43191669e+01  6.82633676e+00 -1.11674562e+01  1.80558865e+00\n",
      "  9.91435304e+00 -1.53563786e+00  1.82868014e+01  2.49644178e+01\n",
      "  1.32250308e+00  4.47711053e+01 -2.02581685e+01 -1.09123877e+01\n",
      " -5.81210337e+00 -4.61138611e+00  8.72077833e+00  1.15623519e+02\n",
      " -2.89902504e+00 -3.66745245e+00 -7.17865604e+01 -3.68475061e+00\n",
      " -7.62599642e+00 -2.84770090e+01  6.55563884e+00  1.64344156e+00\n",
      "  2.79848543e+02 -3.12367611e+01  2.64820002e+01 -1.11209074e+00\n",
      "  1.06881777e+01 -1.10957609e+00  1.26293747e+01 -5.39507104e+01\n",
      "  1.29299122e+01 -1.83977756e-01  7.78282610e+00  1.18158638e+01\n",
      " -7.73495988e+01 -3.60187224e+01  1.07608557e+01 -7.42863388e-01\n",
      "  4.80910988e+00  1.05283985e+01  6.34780135e+00  1.45115043e+00\n",
      " -1.56664123e+00  1.27887947e+01 -2.75053052e+01  3.62179041e+00\n",
      "  8.71893923e+00  9.45403298e+00 -4.76771847e-01  3.43217390e+01\n",
      "  1.11311171e+00 -2.90307850e+01]\n"
     ]
    }
   ],
   "source": [
    "perceptron_exp = Perceptron(66)\n",
    "w_perceptron_exp = perceptron_exp.train(X_train_exp, Y_train, best_epoch_pfe)\n",
    "print('w_exp =', w_perceptron_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a391f88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rate of good answer on train set with expanded features :  0.892875\n",
      "rate of good answer on test set with expanded features :  0.891\n"
     ]
    }
   ],
   "source": [
    "accuracy_train_exp = perceptron_exp.evaluate(X_train_exp, Y_train)\n",
    "accuracy_test_exp = perceptron_exp.evaluate(X_test_exp, Y_test)\n",
    "print('rate of good answer on train set with expanded features : ', accuracy_train_exp)\n",
    "print('rate of good answer on test set with expanded features : ', accuracy_test_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e43c15b",
   "metadata": {},
   "source": [
    "# II- SVM with Pegasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78c5000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    \n",
    "    def __init__(self, feature_number=11):\n",
    "        self.T = None\n",
    "        self.lambda_param = None\n",
    "        self.feature_number = feature_number\n",
    "        self.w = None\n",
    "    \n",
    "    def train(self, X_train, Y_train, T, lambda_param):\n",
    "        \n",
    "        self.T = T                          # T is the number of rounds\n",
    "        self.lambda_param = lambda_param    # lambda is the regularization coefficient\n",
    "        \n",
    "        sum_h = 0\n",
    "        h = np.zeros(self.feature_number)\n",
    "        sample_size = X_train.shape[0]\n",
    "        \n",
    "        for t in range(self.T):\n",
    "            \n",
    "            lr = (1 / (self.lambda_param * (t + 1)))\n",
    "            i = np.random.randint(0, sample_size)\n",
    "            condition = 1 - (Y_train[i] * np.dot(h, X_train[i,:]))\n",
    "            \n",
    "            if (condition > 0):\n",
    "                grad_l = self.lambda_param * h + Y_train[i] * X_train[i,:]\n",
    "            else:\n",
    "                grad_l = self.lambda_param * h\n",
    "                \n",
    "            h = h - lr * grad_l\n",
    "            sum_h = sum_h + h\n",
    "\n",
    "        self.w = (sum_h / self.T)\n",
    "        \n",
    "        return self.w\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        output = np.dot(X_test, self.w)\n",
    "        Y_predicted = np.sign(output)\n",
    "        \n",
    "        return Y_predicted\n",
    "    \n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        \n",
    "        Y_pred = self.predict(X_test)\n",
    "        accuracy = np.mean(Y_pred == Y_test)\n",
    "        \n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "33ba7457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_tuning_parameter(algo, list_T, list_lr, X_train_tuning, Y_train_tuning, X_val_tuning, Y_val_tuning):\n",
    "    \n",
    "    result = np.zeros((len(list_T), len(list_lr)))\n",
    "    for l, t in enumerate(list_T):\n",
    "        for i, lr in enumerate(list_lr):\n",
    "            s = 0\n",
    "            # we repeat it 3 time for each parameter combination in order to get a mean value since pegasos is stochastic\n",
    "            for j in range(3):\n",
    "                algo.train(X_train_tuning, Y_train_tuning, t, lr)\n",
    "                s += algo.evaluate(X_val_tuning, Y_val_tuning)\n",
    "                \n",
    "            result[l,i] = s / 3   \n",
    "\n",
    "    index_max = np.unravel_index(np.argmax(result), result.shape)\n",
    "    best_T = list_T[index_max[0]]\n",
    "    best_lr = list_lr[index_max[1]]\n",
    "    \n",
    "    return best_T, best_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961ca69f",
   "metadata": {},
   "source": [
    "## a) SVM run without feature expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "15a185dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best number of round T :  10000 \n",
      " best learning rate :  0.01\n"
     ]
    }
   ],
   "source": [
    "list_T = [1000, 10000, 50000]\n",
    "list_lr = [0.001, 0.01, 0.1, 1]\n",
    "best_T, best_lr = svm_tuning_parameter(SVM(), list_T, list_lr, X_train_tuning, Y_train_tuning, X_val_tuning, Y_val_tuning)\n",
    "print('best number of round T : ',best_T,'\\n','best lambda : ', best_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "241079d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f32f27c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [-49.54        -9.87690102 -25.70609082 -24.3749704  -37.34486616\n",
      " -27.84897922 -24.88441976 -26.61517831 -26.16444088 -27.79824458\n",
      " -24.70627206]\n"
     ]
    }
   ],
   "source": [
    "w_svm = svm.train(X_train, Y_train, T=best_T, lambda_param=best_lr)\n",
    "print('w =', w_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7215cd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of svm on training set : 0.50125\n",
      "accuracy of svm on test set : 0.499\n"
     ]
    }
   ],
   "source": [
    "accuracy_svm_train = svm.evaluate(X_train, Y_train)\n",
    "accuracy_svm = svm.evaluate(X_test, Y_test)\n",
    "print('accuracy of svm on training set :', accuracy_svm_train)\n",
    "print('accuracy of svm on test set :', accuracy_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dcc06f",
   "metadata": {},
   "source": [
    "## b) SVM run with feature expansion of degree 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6dd0758e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best number of round T feature expansion :  1000 \n",
      " best learning rate with feature expansion :  0.01\n"
     ]
    }
   ],
   "source": [
    "best_T_exp, best_lr_exp = svm_tuning_parameter(SVM(66), list_T, list_lr, X_train_tuning_exp, Y_train_tuning_exp, X_val_tuning_exp, Y_val_tuning_exp)\n",
    "print('best number of round T feature expansion : ',best_T_exp,'\\n','best lambda with feature expansion : ', best_lr_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a613540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_exp = SVM(feature_number=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a13507e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [-50.8        -10.62124289 -26.8768331  -24.98216769 -38.06728142\n",
      " -27.40050099 -25.47601963 -27.55281116 -26.89462393 -28.84960797\n",
      " -25.27840143  -3.44208536 -15.27046921 -16.1873455  -29.9857394\n",
      " -21.47845674 -19.57406723 -15.93746541 -15.130951   -17.56011336\n",
      " -16.72378416  -5.96725364  -5.17849847  -7.7570097   -5.32230392\n",
      "  -5.37950729  -5.99454806  -5.95501127  -6.58288053  -5.31489141\n",
      " -13.33771468 -19.92264046 -14.24469498 -13.3053477  -14.84802982\n",
      " -14.4927237  -15.9462939  -13.24438382 -18.90836621 -13.54964132\n",
      "  -7.42647957 -13.42474738 -13.12731242 -14.27210562  -8.48470345\n",
      " -21.00497157 -18.85231928 -20.54548836 -19.83117241 -21.5627088\n",
      " -18.75373673 -13.63955771 -14.66750776 -14.13721879 -15.14641257\n",
      " -13.60447174 -14.00676352 -13.59643968 -14.34516794 -17.93466622\n",
      " -14.85635585 -15.99478335 -13.85367328 -15.55759428 -13.46415964\n",
      " -14.26678144]\n"
     ]
    }
   ],
   "source": [
    "w_svm_exp = svm_exp.train(X_train_exp, Y_train, T=best_T_exp, lambda_param=best_lr_exp)\n",
    "print('w =', w_svm_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "15de6827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of svm on expanded training set : 0.50125\n",
      "accuracy of svm on expanded test set : 0.499\n"
     ]
    }
   ],
   "source": [
    "accuracy_svm_train_exp = svm_exp.evaluate(X_train_exp, Y_train)\n",
    "accuracy_svm_exp = svm_exp.evaluate(X_test_exp, Y_test)\n",
    "print('accuracy of svm on expanded training set :', accuracy_svm_train_exp)\n",
    "print('accuracy of svm on expanded test set :', accuracy_svm_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7675f57",
   "metadata": {},
   "source": [
    "# III- Regularized logistic classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7429eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regularized_Logistic_Classification:\n",
    "    \n",
    "    def __init__(self, feature_number=11):\n",
    "        self.T = None\n",
    "        self.lambda_param = None\n",
    "        self.feature_number = feature_number        \n",
    "        self.w = None\n",
    "    \n",
    "    def train(self, X_train, Y_train, T, lambda_param):\n",
    "        \n",
    "        self.T = T                          # T is the number of rounds\n",
    "        self.lambda_param = lambda_param    # lambda is the regularization coefficient\n",
    "        sum_h = 0\n",
    "        h = np.zeros(self.feature_number)\n",
    "        sample_size = X_train.shape[0]\n",
    "        \n",
    "        for t in range(self.T):\n",
    "            \n",
    "            lr = (1 / (self.lambda_param * (t + 1)))\n",
    "            i = np.random.randint(0, sample_size)\n",
    "            \n",
    "            num = Y_train[i] * X_train[i,:]\n",
    "            den = np.log(2) * (1 + np.exp(Y_train[i] * np.dot(h, X_train[i,:])))\n",
    "            \n",
    "            grad_l = self.lambda_param * h - (num / den)\n",
    "                \n",
    "            h = h - lr * grad_l\n",
    "            sum_h = sum_h + h\n",
    "\n",
    "        self.w = (sum_h / self.T)\n",
    "        \n",
    "        return self.w\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        output = np.dot(X_test, self.w)\n",
    "        Y_predicted = np.sign(output)\n",
    "        \n",
    "        return Y_predicted\n",
    "    \n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        \n",
    "        Y_pred = self.predict(X_test)\n",
    "        accuracy = np.mean(Y_pred == Y_test)\n",
    "        \n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b2c2ac",
   "metadata": {},
   "source": [
    "## a) Regularized logistic regression run without feature expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b3a2a866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4068/1548082262.py:23: RuntimeWarning: overflow encountered in exp\n",
      "  den = np.log(2) * (1 + np.exp(Y_train[i] * np.dot(h, X_train[i,:])))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best number of round T :  50000 \n",
      " best lambda :  0.001\n"
     ]
    }
   ],
   "source": [
    "list_T = [1000, 10000, 50000]\n",
    "list_lambda = [0.001, 0.01, 0.1, 1]\n",
    "best_T_rlc, best_lambda_rlc = svm_tuning_parameter(Regularized_Logistic_Classification(), list_T, list_lambda, X_train_tuning, Y_train_tuning, X_val_tuning, Y_val_tuning)\n",
    "print('best number of round T : ',best_T_rlc,'\\n','best lambda : ', best_lambda_rlc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2f293cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlc = Regularized_Logistic_Classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "cda63ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4068/1548082262.py:23: RuntimeWarning: overflow encountered in exp\n",
      "  den = np.log(2) * (1 + np.exp(Y_train[i] * np.dot(h, X_train[i,:])))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_rlc = [-1.58410915  2.25098859 -0.96907334 -0.67188579 -2.49473774  0.79264228\n",
      " -0.85464782  2.33751037  5.56664856  0.85450092 -0.60236804] \n",
      "\n",
      "accuracy of rlc on training set : 0.71825\n",
      "accuracy of rlc on test set : 0.7335\n"
     ]
    }
   ],
   "source": [
    "w_rlc = rlc.train(X_train, Y_train, best_T_rlc, best_lambda_rlc)\n",
    "print('w_rlc =', w_rlc, '\\n')\n",
    "accuracy_train_rlc = rlc.evaluate(X_train, Y_train)\n",
    "accuracy_test_rlc = rlc.evaluate(X_test, Y_test)\n",
    "print('accuracy of rlc on training set :', accuracy_train_rlc)\n",
    "print('accuracy of rlc on test set :', accuracy_test_rlc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e4ea40",
   "metadata": {},
   "source": [
    "## b) Regularized logistic regression run with features expansion of degree 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "57bd6833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4068/1548082262.py:23: RuntimeWarning: overflow encountered in exp\n",
      "  den = np.log(2) * (1 + np.exp(Y_train[i] * np.dot(h, X_train[i,:])))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best number of round T with expansion feature for rlc:  50000 \n",
      " best lambda with expansion feature for rlc :  0.001\n"
     ]
    }
   ],
   "source": [
    "best_T_rlc_exp, best_lambda_rlc_exp = svm_tuning_parameter(Regularized_Logistic_Classification(66), list_T, list_lambda, X_train_tuning_exp, Y_train_tuning_exp, X_val_tuning_exp, Y_val_tuning_exp)\n",
    "print('best number of round T with expansion feature for rlc: ', best_T_rlc_exp,'\\n','best lambda with expansion feature for rlc : ', best_lambda_rlc_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "01642328",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlc_exp = Regularized_Logistic_Classification(feature_number=66)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "64d737f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4068/1548082262.py:23: RuntimeWarning: overflow encountered in exp\n",
      "  den = np.log(2) * (1 + np.exp(Y_train[i] * np.dot(h, X_train[i,:])))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of rlc on training set : 0.740125\n",
      "accuracy of rlc on test set : 0.7545\n"
     ]
    }
   ],
   "source": [
    "w_rlc_exp = rlc_exp.train(X_train_exp, Y_train, best_T_rlc_exp, best_lambda_rlc_exp)\n",
    "accuracy_train_rlc_exp = rlc_exp.evaluate(X_train_exp, Y_train)\n",
    "accuracy_test_rlc_exp = rlc_exp.evaluate(X_test_exp, Y_test)\n",
    "print('accuracy of rlc on training set :', accuracy_train_rlc_exp)\n",
    "print('accuracy of rlc on test set :', accuracy_test_rlc_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b37983c",
   "metadata": {},
   "source": [
    "# IV- Kernel Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5e167103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_kernel(X_1, X_2, degree):\n",
    "        \n",
    "    pol_K = (1 + np.matmul(X_1, X_2.T)) ** degree\n",
    "    \n",
    "    return pol_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "81dd7cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(X_1, X_2, gamma):\n",
    "    \n",
    "    X1_norm = np.sum(X_1**2, axis=1).reshape(-1, 1) # create a (1, X_1.shape[0]) vector\n",
    "    X2_norm = np.sum(X_2**2, axis=1).reshape(1, -1) # create a (X_2.shape[0], 1) vector\n",
    "    \n",
    "    # create a (X_1.shape[0], X_2.shape[0]) matrix where component (i, j) is ||X1_i - X2_j||Â² with X1_i the i (line) vector of X1\n",
    "    X_norm = X1_norm + X2_norm - 2 * np.matmul(X_1, X_2.T) \n",
    "    \n",
    "    gaussian_K = np.exp(-(0.5 * X_norm) / gamma)\n",
    "    \n",
    "    return gaussian_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cd38cd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel_Perceptron:\n",
    "    \n",
    "    def __init__(self,  kernel):\n",
    "        self.kernel = kernel\n",
    "        self.max_epoch = None\n",
    "        self.param_kernel = None\n",
    "        self.w_index = None\n",
    "        self.kernel_vectors = None\n",
    "        self.kernel_vector_labels = None\n",
    "    \n",
    "    def train(self, X_train, Y_train, max_epoch, param_kernel):\n",
    "        \n",
    "        sample_size = X_train.shape[0]\n",
    "        self.w_index = np.zeros(sample_size)\n",
    "        self.kernel_vectors = X_train\n",
    "        self.kernel_vector_labels = Y_train\n",
    "        self.max_epoch = max_epoch\n",
    "        self.param_kernel = param_kernel\n",
    "        \n",
    "        K = self.kernel(X_train, X_train, self.param_kernel)\n",
    "        \n",
    "        for epoch in range(self.max_epoch):\n",
    "            \n",
    "            update = False\n",
    "            for i in range(sample_size):\n",
    "                \n",
    "                pred = np.sum(self.w_index * Y_train * K[:, i])\n",
    "                \n",
    "                if (np.sign(pred) != Y_train[i]):\n",
    "                    self.w_index[i] += 1\n",
    "                    update = True\n",
    "\n",
    "            if (update == False):\n",
    "                print('no update in last epoch')\n",
    "                break\n",
    "        \n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        K = self.kernel(X_test, self.kernel_vectors, self.param_kernel)\n",
    "        Y_predicted = np.sign(np.sum(self.w_index * self.kernel_vector_labels * K, axis=1))\n",
    "\n",
    "        return Y_predicted\n",
    "    \n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        \n",
    "        Y_pred = self.predict(X_test)\n",
    "        accuracy = np.mean(Y_pred == Y_test)\n",
    "        \n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1a49d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_perceptron_tuning_parameter(algo, list_epoch, list_kernel_param, X_train_tuning, Y_train_tuning, X_val_tuning, Y_val_tuning):\n",
    "    \n",
    "    result = np.zeros((len(list_epoch), len(list_kernel_param)))\n",
    "    \n",
    "    for l, e in enumerate(list_epoch):\n",
    "        for i, param in enumerate(list_kernel_param):\n",
    "            algo.train(X_train_tuning, Y_train_tuning, e, param)\n",
    "            result[l,i] = algo.evaluate(X_val_tuning, Y_val_tuning)\n",
    "        \n",
    "        print(f'step {l+1} over {len(list_epoch)}')\n",
    "            \n",
    "    index_max = np.unravel_index(np.argmax(result), result.shape)\n",
    "    best_epoch = list_epoch[index_max[0]]\n",
    "    best_param = list_kernel_param[index_max[1]]\n",
    "    \n",
    "    return best_epoch, best_param"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ac86f0",
   "metadata": {},
   "source": [
    "## a) Kernel Perceptron with polynomial kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa96d96",
   "metadata": {},
   "source": [
    "#### Let's try some value for the polynome degree :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2835c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_perceptron_pol = Kernel_Perceptron(polynomial_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e180d54",
   "metadata": {},
   "source": [
    "##### With polynomial degree of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "305d86fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for Perceptron with polynomial kernel of degree 1 :  0.647\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 10\n",
    "polynomial_degree = 1\n",
    "kernel_perceptron_pol.train(X_train, Y_train, max_epoch, polynomial_degree)\n",
    "accuracy_pol_2 = kernel_perceptron_pol.evaluate(X_test, Y_test)\n",
    "print(f'accuracy for Perceptron with polynomial kernel of degree {polynomial_degree} : ', accuracy_pol_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167bed94",
   "metadata": {},
   "source": [
    "##### With polynomial degree of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a01bbfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for Perceptron with polynomial kernel of degree 3 :  0.801\n"
     ]
    }
   ],
   "source": [
    "polynomial_degree = 3\n",
    "kernel_perceptron_pol.train(X_train, Y_train, max_epoch, polynomial_degree)\n",
    "accuracy_pol_3 = kernel_perceptron_pol.evaluate(X_test, Y_test)\n",
    "print(f'accuracy for Perceptron with polynomial kernel of degree {polynomial_degree} : ', accuracy_pol_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0aa386",
   "metadata": {},
   "source": [
    "##### With polynomial degree of 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "475b3d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for Perceptron with polynomial kernel of degree 6 :  0.8965\n"
     ]
    }
   ],
   "source": [
    "polynomial_degree = 6\n",
    "kernel_perceptron_pol.train(X_train, Y_train, max_epoch, polynomial_degree)\n",
    "accuracy_pol_6 = kernel_perceptron_pol.evaluate(X_test, Y_test)\n",
    "print(f'accuracy for Perceptron with polynomial kernel of degree {polynomial_degree} : ', accuracy_pol_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d1797",
   "metadata": {},
   "source": [
    "#### Now, let's try to find the best hyperparameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bcd9c178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "best number of epoch :  41 \n",
      " best polynomial degree :  14\n"
     ]
    }
   ],
   "source": [
    "list_epoch = [i for i in range(1, 51, 10)]\n",
    "list_degree = [i for i in range(1, 16)]\n",
    "best_epoch, best_degree = kernel_perceptron_tuning_parameter(Kernel_Perceptron(polynomial_kernel), list_epoch, list_degree, X_train_tuning, Y_train_tuning, X_val_tuning, Y_val_tuning)\n",
    "print('best number of epoch : ',best_epoch,'\\n','best polynomial degree : ', best_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "bf27f200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on train set for Perceptron with 41 epoch and polynomial kernel of degree 14 :  0.9325\n",
      "accuracy on test set for Perceptron with 41 epoch and polynomial kernel of degree 14 :  0.922\n"
     ]
    }
   ],
   "source": [
    "kernel_perceptron_pol.train(X_train, Y_train, best_epoch, best_degree)\n",
    "accuracy_train_pol = kernel_perceptron_pol.evaluate(X_train, Y_train)\n",
    "accuracy_test_pol = kernel_perceptron_pol.evaluate(X_test, Y_test)\n",
    "print(f'accuracy on train set for Perceptron with {best_epoch} epoch and polynomial kernel of degree {best_degree} : ', accuracy_train_pol)\n",
    "print(f'accuracy on test set for Perceptron with {best_epoch} epoch and polynomial kernel of degree {best_degree} : ', accuracy_test_pol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2be06b",
   "metadata": {},
   "source": [
    "## b) Kernel Perceptron with gaussian kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df39da2b",
   "metadata": {},
   "source": [
    "#### Let's try some value for the gamma parameter :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6e13abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 10\n",
    "kernel_perceptron_gauss = Kernel_Perceptron(gaussian_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cbe9ec",
   "metadata": {},
   "source": [
    "##### with gamma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ac85f242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for Perceptron with gaussian kernel with gamma 1 :  0.852\n"
     ]
    }
   ],
   "source": [
    "gamma = 1\n",
    "kernel_perceptron_gauss.train(X_train, Y_train, max_epoch, gamma)\n",
    "accuracy_gauss_1 = kernel_perceptron_gauss.evaluate(X_test, Y_test)\n",
    "print(f'accuracy for Perceptron with gaussian kernel with gamma {gamma} : ', accuracy_gauss_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f2f526",
   "metadata": {},
   "source": [
    "##### with gamma = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9ad1a6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for Perceptron with gaussian kernel with gamma 2 :  0.9\n"
     ]
    }
   ],
   "source": [
    "gamma = 2\n",
    "kernel_perceptron_gauss.train(X_train, Y_train, max_epoch, gamma)\n",
    "accuracy_gauss_2 = kernel_perceptron_gauss.evaluate(X_test, Y_test)\n",
    "print(f'accuracy for Perceptron with gaussian kernel with gamma {gamma} : ', accuracy_gauss_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a0b670",
   "metadata": {},
   "source": [
    "#### Now, let's try to find the best hyperparameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2fd742fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "best number of epoch :  41 \n",
      " best gamma :  0.1\n"
     ]
    }
   ],
   "source": [
    "list_gamma = [0.1, 0.5, 1, 2, 5, 7, 10]\n",
    "best_epoch, best_gamma = kernel_perceptron_tuning_parameter(Kernel_Perceptron(gaussian_kernel), list_epoch, list_gamma, X_train_tuning, Y_train_tuning, X_val_tuning, Y_val_tuning)\n",
    "print('best number of epoch : ',best_epoch,'\\n','best gamma : ', best_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6fe1b74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on train set for Perceptron with 41 epoch and gaussian kernel with gamma 2 :  0.992625\n",
      "accuracy on test set for Perceptron with 41 epoch and gaussian kernel with 2 :  0.945\n"
     ]
    }
   ],
   "source": [
    "kernel_perceptron_gauss = Kernel_Perceptron(gaussian_kernel)\n",
    "kernel_perceptron_gauss.train(X_train, Y_train, best_epoch, best_gamma)\n",
    "accuracy_train_gauss = kernel_perceptron_gauss.evaluate(X_train, Y_train)\n",
    "accuracy_test_gauss = kernel_perceptron_gauss.evaluate(X_test, Y_test)\n",
    "print(f'accuracy on train set for Perceptron with {best_epoch} epoch and gaussian kernel with gamma {gamma} : ', accuracy_train_gauss)\n",
    "print(f'accuracy on test set for Perceptron with {best_epoch} epoch and gaussian kernel with {gamma} : ', accuracy_test_gauss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d40fb3b",
   "metadata": {},
   "source": [
    "# V- Kernel Pegasos for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2f7f9446",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel_Pegasos:\n",
    "    \n",
    "    def __init__(self, T, kernel):\n",
    "        \n",
    "        self.T = T                          # T is the number of rounds\n",
    "        self.kernel = kernel\n",
    "        self.lambda_param = None\n",
    "        self.kernel_vectors = None\n",
    "        self.kernel_vector_labels = None\n",
    "        self.w_index = None\n",
    "        self.param_kernel = None\n",
    "\n",
    "    \n",
    "    def train(self, X_train, Y_train, lambda_param, param_kernel):\n",
    "        \n",
    "        sample_size = X_train.shape[0]\n",
    "        self.w_index = np.zeros(sample_size)\n",
    "        self.kernel_vectors = X_train\n",
    "        self.kernel_vector_labels = Y_train\n",
    "        self.lambda_param = lambda_param    # lambda is the regularization coefficient\n",
    "        self.param_kernel = param_kernel\n",
    "        \n",
    "        K = self.kernel(X_train, X_train, self.param_kernel)\n",
    "        \n",
    "        for t in range(self.T):\n",
    "            \n",
    "            lr = (1 / (self.lambda_param * (t + 1)))\n",
    "            i = np.random.randint(0, sample_size)\n",
    "            \n",
    "            condition = Y_train[i] * lr * np.sum(self.w_index * Y_train * K[:, i])\n",
    "            \n",
    "            if (condition < 1):\n",
    "                self.w_index[i] += 1\n",
    "            \n",
    "\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        K = self.kernel(X_test, self.kernel_vectors, self.param_kernel)\n",
    "        Y_predicted = np.sign(np.sum(self.w_index * self.kernel_vector_labels * K, axis=1))\n",
    "\n",
    "        return Y_predicted\n",
    "    \n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        \n",
    "        Y_pred = self.predict(X_test)\n",
    "        accuracy = np.mean(Y_pred == Y_test)\n",
    "        \n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "64dcbd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_pegasos_tuning_parameter(algo, list_lr, list_kernel_param, X_train_tuning, Y_train_tuning, X_val_tuning, Y_val_tuning):\n",
    "    \n",
    "    result = np.zeros((len(list_lr), len(list_kernel_param)))\n",
    "    \n",
    "    for l, e in enumerate(list_lr):\n",
    "        for i, param in enumerate(list_kernel_param):\n",
    "            algo.train(X_train_tuning, Y_train_tuning, e, param)\n",
    "            result[l,i] = algo.evaluate(X_val_tuning, Y_val_tuning)\n",
    "        print(f'step {l+1} over {len(list_lr)}')\n",
    "            \n",
    "    index_max = np.unravel_index(np.argmax(result), result.shape)\n",
    "    best_lr = list_lr[index_max[0]]\n",
    "    best_param = list_kernel_param[index_max[1]]\n",
    "    \n",
    "    return best_lr, best_param"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc38888",
   "metadata": {},
   "source": [
    "## a) Kernel Pegasos with polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c782c74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "best lambda :  0.1 \n",
      " best polynomial degree :  14\n"
     ]
    }
   ],
   "source": [
    "T = 30000  # we fix T big enough in hope to get convergence\n",
    "list_degree = [i for i in range(1, 16)]\n",
    "list_lambda = [0.001, 0.01, 0.1, 1]\n",
    "best_lambda, best_degree = kernel_pegasos_tuning_parameter(Kernel_Pegasos(T,polynomial_kernel), list_lambda, list_degree, X_train_tuning, Y_train_tuning, X_val_tuning, Y_val_tuning)\n",
    "print('best lambda : ', best_lambda ,'\\n','best polynomial degree : ', best_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f607da8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on train set for Pegasos with 0.1 lambda and polynomial kernel of degree 14 :  0.867875\n",
      "accuracy on test set for Pegasos with 0.1 lambda and polynomial kernel of degree 14 :  0.8705\n"
     ]
    }
   ],
   "source": [
    "kernel_pegasos_pol = Kernel_Pegasos(T, polynomial_kernel)\n",
    "kernel_pegasos_pol.train(X_train, Y_train, best_epoch, best_degree)\n",
    "accuracy_train_pol = kernel_pegasos_pol.evaluate(X_train, Y_train)\n",
    "accuracy_test_pol = kernel_pegasos_pol.evaluate(X_test, Y_test)\n",
    "print(f'accuracy on train set for Pegasos with lambda = {best_lambda} and polynomial kernel of degree {best_degree} : ', accuracy_train_pol)\n",
    "print(f'accuracy on test set for Pegasos with lambda = {best_lambda} and polynomial kernel of degree {best_degree} : ', accuracy_test_pol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ae45f8",
   "metadata": {},
   "source": [
    "## b) Kernel Pegasos with gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "20091150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "best lambda :  0.001 \n",
      " best gamma :  0.1\n"
     ]
    }
   ],
   "source": [
    "T = 30000  # we fix T big enough in hope to get convergence\n",
    "list_gamma = [0.1, 0.5, 1, 2, 5, 7, 10]\n",
    "list_lambda = [0.001, 0.01, 0.1, 1]\n",
    "best_lambda, best_gamma = kernel_pegasos_tuning_parameter(Kernel_Pegasos(T,gaussian_kernel), list_lambda, list_gamma, X_train_tuning, Y_train_tuning, X_val_tuning, Y_val_tuning)\n",
    "print('best lambda : ', best_lambda ,'\\n','best gamma : ', best_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "48237841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on train set for Pegasos with lambda = 0.001 and gaussian kernel of gamma 0.1 :  0.8675\n",
      "accuracy on test set for Pegasos with lambda = 0.001 and gaussian kernel with gamma 0.1 :  0.863\n"
     ]
    }
   ],
   "source": [
    "kernel_pegasos_gauss = Kernel_Pegasos(T, gaussian_kernel)\n",
    "kernel_pegasos_gauss.train(X_train, Y_train, best_lambda, best_gamma)\n",
    "accuracy_train_gauss = kernel_pegasos_gauss.evaluate(X_train, Y_train)\n",
    "accuracy_test_gauss = kernel_pegasos_gauss.evaluate(X_test, Y_test)\n",
    "print(f'accuracy on train set for Pegasos with lambda = {best_lambda} and gaussian kernel of gamma {best_gamma} : ', accuracy_train_gauss)\n",
    "print(f'accuracy on test set for Pegasos with lambda = {best_lambda} and gaussian kernel with gamma {best_gamma} : ', accuracy_test_gauss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
